{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / np.max(x_train)\n",
    "x_test = x_test / np.max(x_test)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 235,146\n",
      "Trainable params: 235,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.5226 - acc: 0.8371 - val_loss: 0.1692 - val_acc: 0.9491\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.2413 - acc: 0.9301 - val_loss: 0.1246 - val_acc: 0.9623\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1930 - acc: 0.9432 - val_loss: 0.1055 - val_acc: 0.9685\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1654 - acc: 0.9515 - val_loss: 0.0965 - val_acc: 0.9714\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1457 - acc: 0.9565 - val_loss: 0.0926 - val_acc: 0.9727\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.1327 - acc: 0.9618 - val_loss: 0.0798 - val_acc: 0.9756\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1250 - acc: 0.9630 - val_loss: 0.0808 - val_acc: 0.9750\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1180 - acc: 0.9655 - val_loss: 0.0770 - val_acc: 0.9771\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1082 - acc: 0.9679 - val_loss: 0.0757 - val_acc: 0.9786\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.1023 - acc: 0.9686 - val_loss: 0.0739 - val_acc: 0.9796\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIsAAAEyCAYAAAB6clB0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxv4Nj/7VxrLwT3IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2294698278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.073946045593731108, 0.97960000000000003]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = pd.DataFrame([history.history['acc'], history.history['val_acc']])\n",
    "h = h.T\n",
    "h.columns=['acc', 'val_acc']\n",
    "h.plot(figsize=(20, 5))\n",
    "plt.show()\n",
    "model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, a convolutional network\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / np.max(x_train)\n",
    "x_test = x_test / np.max(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 3 from 1 for 'conv2d_18/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    653\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_18/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-d6a6b544fad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                      \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                      input_shape=(1, 28, 28)))\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m                 \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m   3136\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3137\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3138\u001b[0;31m         data_format='NHWC')\n\u001b[0m\u001b[1;32m   3139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_postprocess_conv2d_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution\u001b[0;34m(input, filter, padding, strides, dilation_rate, name, data_format)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         op=op)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mwith_space_to_batch\u001b[0;34m(input, dilation_rate, padding, op, filter_shape, spatial_dims, data_format)\u001b[0m\n\u001b[1;32m    336\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dilation_rate must be positive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconst_rate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_spatial_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0;31m# We have two padding contributions. The first is used for converting \"SAME\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mop\u001b[0;34m(input_converted, _, padding)\u001b[0m\n\u001b[1;32m    662\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m           \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     return with_space_to_batch(\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_non_atrous_convolution\u001b[0;34m(input, filter, padding, data_format, strides, name)\u001b[0m\n\u001b[1;32m    129\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mconv_dims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NDHWC\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[1;32m    395\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m    398\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2631\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2632\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_18/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32]."
     ]
    }
   ],
   "source": [
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv2D(32, \n",
    "                     kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(28, 28, 1)))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_cnn.add(Dropout(0.25))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(512, activation='relu'))\n",
    "model_cnn.add(Dropout(0.2))\n",
    "model_cnn.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_7_input to have 2 dimensions, but got array with shape (60000, 28, 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-7156bfcdf770>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1435\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1436\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1309\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1312\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1313\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_7_input to have 2 dimensions, but got array with shape (60000, 28, 28)"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=10,\n",
    "          epochs=32,\n",
    "          verbose=1,\n",
    "validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
