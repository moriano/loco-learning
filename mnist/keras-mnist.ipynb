{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / np.max(x_train)\n",
    "x_test = x_test / np.max(x_test)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.2732 - acc: 0.9206 - val_loss: 0.1329 - val_acc: 0.9596\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.1175 - acc: 0.9650 - val_loss: 0.0903 - val_acc: 0.9716\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.0813 - acc: 0.9748 - val_loss: 0.0761 - val_acc: 0.9755\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s - loss: 0.0639 - acc: 0.9809 - val_loss: 0.0729 - val_acc: 0.9780\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s - loss: 0.0521 - acc: 0.9845 - val_loss: 0.0709 - val_acc: 0.9782\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s - loss: 0.0431 - acc: 0.9865 - val_loss: 0.0615 - val_acc: 0.9817\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 7s - loss: 0.0367 - acc: 0.9890 - val_loss: 0.0622 - val_acc: 0.9828\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s - loss: 0.0310 - acc: 0.9904 - val_loss: 0.0673 - val_acc: 0.9815\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.0274 - acc: 0.9912 - val_loss: 0.0676 - val_acc: 0.9811\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.0239 - acc: 0.9926 - val_loss: 0.0717 - val_acc: 0.9807\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIsAAAEyCAYAAAB6clB0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOY/MqIagbwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb1c582cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.071672378940045023, 0.98070000000000002]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = pd.DataFrame([history.history['acc'], history.history['val_acc']])\n",
    "h = h.T\n",
    "h.columns=['acc', 'val_acc']\n",
    "h.plot(figsize=(20, 5))\n",
    "plt.show()\n",
    "model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, a convolutional network\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / np.max(x_train)\n",
    "x_test = x_test / np.max(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 3 from 1 for 'conv2d_18/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    653\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_18/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-d6a6b544fad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                      \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                      input_shape=(1, 28, 28)))\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m                 \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m   3136\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3137\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3138\u001b[0;31m         data_format='NHWC')\n\u001b[0m\u001b[1;32m   3139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_postprocess_conv2d_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution\u001b[0;34m(input, filter, padding, strides, dilation_rate, name, data_format)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         op=op)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mwith_space_to_batch\u001b[0;34m(input, dilation_rate, padding, op, filter_shape, spatial_dims, data_format)\u001b[0m\n\u001b[1;32m    336\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dilation_rate must be positive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconst_rate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_spatial_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0;31m# We have two padding contributions. The first is used for converting \"SAME\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mop\u001b[0;34m(input_converted, _, padding)\u001b[0m\n\u001b[1;32m    662\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m           \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     return with_space_to_batch(\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_non_atrous_convolution\u001b[0;34m(input, filter, padding, data_format, strides, name)\u001b[0m\n\u001b[1;32m    129\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mconv_dims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NDHWC\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[1;32m    395\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m    398\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2631\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2632\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_18/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32]."
     ]
    }
   ],
   "source": [
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv2D(32, \n",
    "                     kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(28, 28, 1)))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_cnn.add(Dropout(0.25))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(512, activation='relu'))\n",
    "model_cnn.add(Dropout(0.2))\n",
    "model_cnn.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_7_input to have 2 dimensions, but got array with shape (60000, 28, 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-7156bfcdf770>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1435\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1436\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1309\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1312\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1313\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tools/anaconda3/envs/tensorflow1.2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_7_input to have 2 dimensions, but got array with shape (60000, 28, 28)"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=10,\n",
    "          epochs=32,\n",
    "          verbose=1,\n",
    "validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
