{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plant seedlings classification with bottleneck features. \n",
    "\n",
    "See plant-seedlings-classification-transferLearning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os import mkdir\n",
    "from os import makedirs\n",
    "import os\n",
    "import shutil\n",
    "from IPython.display import Image, display\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import vgg16, vgg19\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This code will require GPU usage... so sometimes we will need to run it in floydhub\n",
    "FLOYDHUB=True\n",
    "if FLOYDHUB:\n",
    "    OUTPUT_DIR = \"/output/\"\n",
    "    TRAIN_DIR = \"/input/train/\"\n",
    "    VALIDATION_DIR = \"/input/validation/\"\n",
    "    FAKE_TEST_DIR = \"/input/fake-test\"\n",
    "    TEST_DIR = \"/input/test\"\n",
    "else:\n",
    "    OUTPUT_DIR = \"/tmp/\"\n",
    "    TRAIN_DIR = \"train/\"\n",
    "    VALIDATION_DIR = \"validation/\"\n",
    "    FAKE_TEST_DIR = \"fake-test/\"\n",
    "    TEST_DIR = \"test/\"\n",
    "\n",
    "# As per the image size we will use, I am going with 224... no particular reason really\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\n",
    "    \"Black-grass\",\n",
    "    \"Charlock\",\n",
    "    \"Cleavers\",\n",
    "    \"Common Chickweed\",\n",
    "    \"Common wheat\",\n",
    "    \"Fat Hen\",\n",
    "    \"Loose Silky-bent\",\n",
    "    \"Maize\",\n",
    "    \"Scentless Mayweed\",\n",
    "    \"Shepherds Purse\",\n",
    "    \"Small-flowered Cranesbill\",\n",
    "    \"Sugar beet\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 images so far...\n",
      "Loaded 200 images so far...\n",
      "Loaded 300 images so far...\n",
      "Loaded 400 images so far...\n",
      "Loaded 500 images so far...\n",
      "Loaded 600 images so far...\n",
      "Loaded 700 images so far...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "real_test_images = []\n",
    "final_predictions = pd.DataFrame(columns=CLASS_NAMES)\n",
    "\n",
    "image_files = listdir(TEST_DIR)\n",
    "i = 0\n",
    "for image_file in image_files:     \n",
    "    raw_image = io.imread(TEST_DIR+\"/\"+image_file)\n",
    "    scaled_img = cv2.resize(raw_image, (IMAGE_WIDTH, IMAGE_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "    real_test_images.append(scaled_img)\n",
    "    i+=1    \n",
    "    if i % 100 == 0:\n",
    "        print(\"Loaded\", i, \"images so far...\")\n",
    "X = np.array(real_test_images)\n",
    "X = X / 255\n",
    "print(\"Done!\") \n",
    "\n",
    "\n",
    "def predict_and_dump(model_to_use, X_to_use, image_files_to_use, file_name):\n",
    "    results = model_to_use.predict(X_to_use, verbose=1)\n",
    "    final_predictions = pd.DataFrame(columns=CLASS_NAMES, data=results)\n",
    "    predictions = final_predictions.head().idxmax(axis=1)\n",
    "    kaggle_data = pd.DataFrame(columns=[\"file\"])\n",
    "    kaggle_data[\"file\"] = image_files_to_use\n",
    "    kaggle_data[\"species\"] = final_predictions.idxmax(axis=1)\n",
    "    kaggle_data.to_csv(file_name, index=False)\n",
    "    return kaggle_data, final_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3806 images belonging to 12 classes.\n",
      "Found 474 images belonging to 12 classes.\n",
      "Found 470 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False)\n",
    "\n",
    "# this is the augmentation configuration we will use for validation:\n",
    "# only rescaling\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# and the same for the test set\n",
    "fake_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_DIR,  # this is the target directory\n",
    "        target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # all images will be resized\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None, # this means our generator will only yield batches of data, no labels\n",
    "        shuffle=False) # It is very iimportant NOT to shuffle the data, as we need them in order...\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        VALIDATION_DIR,\n",
    "        target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)  \n",
    "\n",
    "# And the generator for test data\n",
    "fake_test_generator = fake_test_datagen.flow_from_directory(\n",
    "        FAKE_TEST_DIR,\n",
    "        target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_samples = (3805 // batch_size ) * batch_size\n",
    "validation_samples = (474 // batch_size ) * batch_size\n",
    "fake_test_samples = (470 // batch_size ) * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = vgg16.VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_2 \t NOT trainable\n",
      "block1_conv1 \t trainable\n",
      "block1_conv2 \t trainable\n",
      "block1_pool \t trainable\n",
      "block2_conv1 \t trainable\n",
      "block2_conv2 \t trainable\n",
      "block2_pool \t trainable\n",
      "block3_conv1 \t trainable\n",
      "block3_conv2 \t trainable\n",
      "block3_conv3 \t trainable\n",
      "block3_pool \t trainable\n",
      "block4_conv1 \t trainable\n",
      "block4_conv2 \t trainable\n",
      "block4_conv3 \t trainable\n",
      "block4_pool \t trainable\n",
      "block5_conv1 \t trainable\n",
      "block5_conv2 \t trainable\n",
      "block5_conv3 \t trainable\n",
      "block5_pool \t trainable\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, \"\\t\",  \"trainable\" if layer.trainable else \"NOT trainable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237/237 [==============================] - 54s    \n"
     ]
    }
   ],
   "source": [
    "bottleneck_features_train = model.predict_generator(train_generator, \n",
    "                                                          training_samples // batch_size, \n",
    "                                                          verbose=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 6s     \n"
     ]
    }
   ],
   "source": [
    "bottleneck_features_validation = model.predict_generator(validation_generator, \n",
    "                                                               validation_samples // batch_size,\n",
    "                                                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 6s     \n"
     ]
    }
   ],
   "source": [
    "bottleneck_features_fake_test = model.predict_generator(fake_test_generator, \n",
    "                                                               fake_test_samples // batch_size,\n",
    "                                                               verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice that what we will need are the bottleneck_ variables as INPUTS for our new model, that's why they are saved into disk..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.save(OUTPUT_DIR+'bottleneck_features_train.npy', bottleneck_features_train)\n",
    "np.save(OUTPUT_DIR+'bottleneck_features_validation.npy', bottleneck_features_validation)\n",
    "np.save(OUTPUT_DIR+'bottleneck_features_fake_test.npy', bottleneck_features_fake_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3792, 7, 7, 512)\n",
      "(464, 7, 7, 512)\n",
      "(464, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "print(bottleneck_features_train.shape)\n",
    "print(bottleneck_features_validation.shape)\n",
    "print(bottleneck_features_fake_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also need to get the labels for the features we have loaded... and we need to make sure we get them in the same order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3806 images belonging to 12 classes.\n",
      "Found 474 images belonging to 12 classes.\n",
      "Found 470 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "# This is the same as the train generator... but it will have the classes too.\n",
    "train_generator_classes = train_datagen.flow_from_directory(\n",
    "                            TRAIN_DIR,  # this is the target directory\n",
    "                            target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # all images will be resized to 150x150\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            class_mode='categorical')  # since we use categorical_crossentropy loss, \n",
    "                                                       # we will need one-hot-encoded...\n",
    "    \n",
    "# This is the same as the validation generator... but it will have the classes too\n",
    "validation_generator_classes = validation_datagen.flow_from_directory(\n",
    "                            VALIDATION_DIR,\n",
    "                            target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            class_mode='categorical')  # since we use categorical_crossentropy loss, \n",
    "                                                       # we will need one-hot-encoded...  \n",
    "    \n",
    "    \n",
    "fake_test_generator_classes = fake_test_datagen.flow_from_directory(\n",
    "                            FAKE_TEST_DIR ,\n",
    "                            target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            class_mode='categorical')  # since we use categorical_crossentropy loss, \n",
    "                                                       # we will need one-hot-encoded...      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we used variables\n",
    "\n",
    "    training_samples\n",
    "    validation_samples\n",
    "\n",
    "before to determine how many images will be for training/validation, lets use the same ones to restore the labels\n",
    "\n",
    "This is a bit non-direct, but here's how it works. Essentially we will iterate over the train_generator_classes variable a total of\n",
    "\n",
    "training_samples // batch_size\n",
    "\n",
    "And the same will be done with the validation_generator_classes, that will yield us X and y elements, we can then use the y element in conjunction with classes and class_indices attributes to build a numpy array with the correct labels\n",
    "\n",
    "Also, regarding how the array is constructed, you REALLY want to check this http://akuederle.com/create-numpy-array-with-for-loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottleneck_features_validation shape is  (3792, 7, 7, 512)\n",
      "200 to go\n",
      "100 to go\n",
      "0 to go\n",
      "train_labels shape is (3792, 12)\n",
      "bottleneck_features_validation shape is  (464, 7, 7, 512)\n",
      "0 to go\n",
      "validation_labels shape is (464, 12)\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(CLASS_NAMES)\n",
    "print(\"bottleneck_features_validation shape is \", bottleneck_features_train.shape)\n",
    "train_labels = np.empty((0, num_classes))\n",
    "total_iterations = training_samples // batch_size\n",
    "for x, y in train_generator_classes:\n",
    "    train_labels = np.append(train_labels, y, axis=0)\n",
    "    total_iterations-=1\n",
    "    if total_iterations % 100 == 0:\n",
    "        print(total_iterations, \"to go\")\n",
    "    if total_iterations == 0:\n",
    "        break\n",
    "        \n",
    "print(\"train_labels shape is\", train_labels.shape)\n",
    "\n",
    "\n",
    "print(\"bottleneck_features_validation shape is \", bottleneck_features_validation.shape)\n",
    "validation_labels = np.empty((0, num_classes))\n",
    "total_iterations = validation_samples // batch_size\n",
    "for x, y in validation_generator_classes:\n",
    "    validation_labels = np.append(validation_labels, y, axis=0)\n",
    "    total_iterations-=1\n",
    "    if total_iterations % 100 == 0:\n",
    "        print(total_iterations, \"to go\")\n",
    "    if total_iterations == 0:\n",
    "        break\n",
    "        \n",
    "print(\"validation_labels shape is\", validation_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_test_features_validation shape is  (464, 7, 7, 512)\n",
      "0 to go\n",
      "fake_test_labels shape is (464, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"fake_test_features_validation shape is \", bottleneck_features_fake_test.shape)\n",
    "fake_test_labels = np.empty((0, num_classes))\n",
    "total_iterations = fake_test_samples // batch_size\n",
    "for x, y in fake_test_generator_classes:\n",
    "    fake_test_labels = np.append(fake_test_labels, y, axis=0)\n",
    "    total_iterations-=1\n",
    "    if total_iterations % 100 == 0:\n",
    "        print(total_iterations, \"to go\")\n",
    "    if total_iterations == 0:\n",
    "        break\n",
    "        \n",
    "print(\"fake_test_labels shape is\", fake_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                12300     \n",
      "=================================================================\n",
      "Total params: 25,703,436\n",
      "Trainable params: 25,703,436\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=bottleneck_features_train.shape[1:]))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.75))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3792 samples, validate on 464 samples\n",
      "Epoch 1/50\n",
      "3792/3792 [==============================] - 8s - loss: 2.3553 - acc: 0.2273 - val_loss: 1.9199 - val_acc: 0.4052\n",
      "Epoch 2/50\n",
      "3792/3792 [==============================] - 8s - loss: 1.9513 - acc: 0.3368 - val_loss: 1.7084 - val_acc: 0.4892\n",
      "Epoch 3/50\n",
      "3792/3792 [==============================] - 8s - loss: 1.7680 - acc: 0.3979 - val_loss: 1.5524 - val_acc: 0.5172\n",
      "Epoch 4/50\n",
      "3792/3792 [==============================] - 8s - loss: 1.5971 - acc: 0.4668 - val_loss: 1.4121 - val_acc: 0.5647\n",
      "Epoch 5/50\n",
      "3792/3792 [==============================] - 8s - loss: 1.4881 - acc: 0.4916 - val_loss: 1.3414 - val_acc: 0.5754\n",
      "Epoch 6/50\n",
      "3792/3792 [==============================] - 8s - loss: 1.3913 - acc: 0.5322 - val_loss: 1.2935 - val_acc: 0.5819\n",
      "Epoch 7/50\n",
      "3792/3792 [==============================] - 8s - loss: 1.3106 - acc: 0.5638 - val_loss: 1.2055 - val_acc: 0.6422\n",
      "Epoch 8/50\n",
      "3792/3792 [==============================] - 8s - loss: 1.2444 - acc: 0.5849 - val_loss: 1.1784 - val_acc: 0.6509\n",
      "Epoch 9/50\n",
      "3792/3792 [==============================] - 8s - loss: 1.1727 - acc: 0.6108 - val_loss: 1.1221 - val_acc: 0.6487\n",
      "Epoch 10/50\n",
      "3792/3792 [==============================] - 8s - loss: 1.1033 - acc: 0.6421 - val_loss: 1.0797 - val_acc: 0.6659\n",
      "Epoch 11/50\n",
      "3792/3792 [==============================] - 8s - loss: 1.0513 - acc: 0.6522 - val_loss: 1.0283 - val_acc: 0.6897\n",
      "Epoch 12/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.9793 - acc: 0.6756 - val_loss: 1.0156 - val_acc: 0.6810\n",
      "Epoch 13/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.9624 - acc: 0.6806 - val_loss: 0.9650 - val_acc: 0.6940\n",
      "Epoch 14/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.9157 - acc: 0.7002 - val_loss: 0.9448 - val_acc: 0.7112\n",
      "Epoch 15/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.8750 - acc: 0.7046 - val_loss: 0.9483 - val_acc: 0.7004\n",
      "Epoch 16/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.8531 - acc: 0.7168 - val_loss: 0.9137 - val_acc: 0.7091\n",
      "Epoch 17/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.8181 - acc: 0.7389 - val_loss: 0.8781 - val_acc: 0.7241\n",
      "Epoch 18/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.7881 - acc: 0.7395 - val_loss: 0.8702 - val_acc: 0.7177\n",
      "Epoch 19/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.7619 - acc: 0.7460 - val_loss: 0.8683 - val_acc: 0.7220\n",
      "Epoch 20/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.7302 - acc: 0.7621 - val_loss: 0.8762 - val_acc: 0.7155\n",
      "Epoch 21/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.6998 - acc: 0.7729 - val_loss: 0.8296 - val_acc: 0.7457\n",
      "Epoch 22/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.6810 - acc: 0.7716 - val_loss: 0.8308 - val_acc: 0.7306\n",
      "Epoch 23/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.6644 - acc: 0.7803 - val_loss: 0.8371 - val_acc: 0.7263\n",
      "Epoch 24/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.6464 - acc: 0.7880 - val_loss: 0.8331 - val_acc: 0.7435\n",
      "Epoch 25/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.6240 - acc: 0.7927 - val_loss: 0.8212 - val_acc: 0.7328\n",
      "Epoch 26/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.6119 - acc: 0.7948 - val_loss: 0.8362 - val_acc: 0.7198\n",
      "Epoch 27/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.5957 - acc: 0.8025 - val_loss: 0.7925 - val_acc: 0.7522\n",
      "Epoch 28/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.5811 - acc: 0.8122 - val_loss: 0.8058 - val_acc: 0.7522\n",
      "Epoch 29/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.5534 - acc: 0.8178 - val_loss: 0.7910 - val_acc: 0.7349\n",
      "Epoch 30/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.5409 - acc: 0.8167 - val_loss: 0.7842 - val_acc: 0.7522\n",
      "Epoch 31/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.5326 - acc: 0.8212 - val_loss: 0.7761 - val_acc: 0.7651\n",
      "Epoch 32/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.5214 - acc: 0.8341 - val_loss: 0.8069 - val_acc: 0.7435\n",
      "Epoch 33/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.5072 - acc: 0.8336 - val_loss: 0.7817 - val_acc: 0.7500\n",
      "Epoch 34/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.5102 - acc: 0.8389 - val_loss: 0.7708 - val_acc: 0.7651\n",
      "Epoch 35/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.4722 - acc: 0.8499 - val_loss: 0.7650 - val_acc: 0.7651\n",
      "Epoch 36/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.4764 - acc: 0.8457 - val_loss: 0.7484 - val_acc: 0.7651\n",
      "Epoch 37/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.4490 - acc: 0.8557 - val_loss: 0.7522 - val_acc: 0.7716\n",
      "Epoch 38/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.4464 - acc: 0.8571 - val_loss: 0.7747 - val_acc: 0.7694\n",
      "Epoch 39/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.4256 - acc: 0.8629 - val_loss: 0.7592 - val_acc: 0.7694\n",
      "Epoch 40/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.4304 - acc: 0.8616 - val_loss: 0.7857 - val_acc: 0.7651\n",
      "Epoch 41/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.4243 - acc: 0.8639 - val_loss: 0.7790 - val_acc: 0.7759\n",
      "Epoch 42/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.4016 - acc: 0.8710 - val_loss: 0.7733 - val_acc: 0.7672\n",
      "Epoch 43/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.3949 - acc: 0.8652 - val_loss: 0.7643 - val_acc: 0.7629\n",
      "Epoch 44/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.3942 - acc: 0.8724 - val_loss: 0.7657 - val_acc: 0.7586\n",
      "Epoch 45/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.3851 - acc: 0.8771 - val_loss: 0.7765 - val_acc: 0.7608\n",
      "Epoch 46/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.3835 - acc: 0.8700 - val_loss: 0.7547 - val_acc: 0.7694\n",
      "Epoch 47/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.3495 - acc: 0.8869 - val_loss: 0.7794 - val_acc: 0.7586\n",
      "Epoch 48/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.3592 - acc: 0.8776 - val_loss: 0.7729 - val_acc: 0.7672\n",
      "Epoch 49/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.3730 - acc: 0.8750 - val_loss: 0.7841 - val_acc: 0.7629\n",
      "Epoch 50/50\n",
      "3792/3792 [==============================] - 8s - loss: 0.3483 - acc: 0.8819 - val_loss: 0.7560 - val_acc: 0.7716\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(bottleneck_features_train, train_labels,\n",
    "                          epochs=50,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=(bottleneck_features_validation, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(OUTPUT_DIR+'bottleneck_fc_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "vgg16_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=vgg16_model.output_shape[1:]))\n",
    "top_model.add(Dense(1024, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(12, activation=\"softmax\"))\n",
    "top_model.load_weights(OUTPUT_DIR+'bottleneck_fc_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fine_tuned_model = Model(inputs=vgg16_model.input, outputs=top_model(vgg16_model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_3 (Sequential)    (None, 12)                25703436  \n",
      "=================================================================\n",
      "Total params: 40,418,124\n",
      "Trainable params: 40,418,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for layer in fine_tuned_model.layers[:-5]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "Layer  input_4 \t is NOT trainable\n",
      "Layer  block1_conv1 \t is NOT trainable\n",
      "Layer  block1_conv2 \t is NOT trainable\n",
      "Layer  block1_pool \t is NOT trainable\n",
      "Layer  block2_conv1 \t is NOT trainable\n",
      "Layer  block2_conv2 \t is NOT trainable\n",
      "Layer  block2_pool \t is NOT trainable\n",
      "Layer  block3_conv1 \t is NOT trainable\n",
      "Layer  block3_conv2 \t is NOT trainable\n",
      "Layer  block3_conv3 \t is NOT trainable\n",
      "Layer  block3_pool \t is NOT trainable\n",
      "Layer  block4_conv1 \t is NOT trainable\n",
      "Layer  block4_conv2 \t is NOT trainable\n",
      "Layer  block4_conv3 \t is NOT trainable\n",
      "Layer  block4_pool \t is NOT trainable\n",
      "Layer  block5_conv1 \t is trainable\n",
      "Layer  block5_conv2 \t is trainable\n",
      "Layer  block5_conv3 \t is trainable\n",
      "Layer  block5_pool \t is trainable\n",
      "Layer  sequential_3 \t is trainable\n"
     ]
    }
   ],
   "source": [
    "print(len(fine_tuned_model.layers))\n",
    "for layer in fine_tuned_model.layers:\n",
    "    if layer.trainable:\n",
    "        print(\"Layer \", layer.name, \"\\t is trainable\")\n",
    "    else:\n",
    "        print(\"Layer \", layer.name, \"\\t is NOT trainable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3806 images belonging to 12 classes.\n",
      "Found 474 images belonging to 12 classes.\n",
      "Found 470 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=180,\n",
    "        width_shift_range=0.5, # Consider removing...\n",
    "        height_shift_range=0.5,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True)\n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for validation:\n",
    "# only rescaling\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# and the same for the test set\n",
    "fake_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_DIR,  # this is the target directory\n",
    "        target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use categorical_crossentropy loss, we will need one-hot-encoded...\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        VALIDATION_DIR,\n",
    "        target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# And the generator for test data\n",
    "fake_test_generator = fake_test_datagen.flow_from_directory(\n",
    "        FAKE_TEST_DIR,\n",
    "        target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "237/237 [==============================] - 84s - loss: 1.9933 - acc: 0.3431 - val_loss: 1.1776 - val_acc: 0.6078\n",
      "Epoch 2/10\n",
      "237/237 [==============================] - 81s - loss: 1.3929 - acc: 0.5229 - val_loss: 0.8597 - val_acc: 0.7162\n",
      "Epoch 3/10\n",
      "237/237 [==============================] - 81s - loss: 1.1221 - acc: 0.6119 - val_loss: 0.9409 - val_acc: 0.7140\n",
      "Epoch 4/10\n",
      "237/237 [==============================] - 81s - loss: 0.9683 - acc: 0.6635 - val_loss: 0.9232 - val_acc: 0.7227\n",
      "Epoch 5/10\n",
      "237/237 [==============================] - 81s - loss: 0.8682 - acc: 0.7018 - val_loss: 0.6899 - val_acc: 0.7795\n",
      "Epoch 6/10\n",
      "237/237 [==============================] - 81s - loss: 0.7919 - acc: 0.7302 - val_loss: 0.6330 - val_acc: 0.8013\n",
      "Epoch 7/10\n",
      "237/237 [==============================] - 81s - loss: 0.7395 - acc: 0.7462 - val_loss: 0.7148 - val_acc: 0.7860\n",
      "Epoch 8/10\n",
      "237/237 [==============================] - 80s - loss: 0.6705 - acc: 0.7641 - val_loss: 0.5302 - val_acc: 0.8231\n",
      "Epoch 9/10\n",
      "237/237 [==============================] - 80s - loss: 0.6871 - acc: 0.7647 - val_loss: 0.5973 - val_acc: 0.8188\n",
      "Epoch 10/10\n",
      "237/237 [==============================] - 80s - loss: 0.6579 - acc: 0.7736 - val_loss: 0.5458 - val_acc: 0.8297\n"
     ]
    }
   ],
   "source": [
    "adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "fine_tuned_model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = fine_tuned_model.fit_generator(train_generator,\n",
    "                                                  steps_per_epoch=training_samples // batch_size,\n",
    "                                                  epochs=10,\n",
    "                                                  validation_data=validation_generator,\n",
    "                                                  validation_steps=validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.48653128897321635, 0.83620689655172409]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model.evaluate_generator(fake_test_generator, steps= fake_test_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
