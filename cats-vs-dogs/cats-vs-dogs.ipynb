{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cats vs dogs\n",
    "\n",
    "## Convnet example\n",
    "\n",
    "### Data\n",
    "\n",
    "The data is provided by kaggle, unfortunately the download link is not public, you will need to create an account at kaggle.com (it is free) and then download\n",
    "\n",
    "https://www.kaggle.com/c/dogs-vs-cats/download/train.zip\n",
    "\n",
    "Note that they file name is \"train.zip\" and it contains all the labeled data, we will need to split that into our train, validation and test set.\n",
    "\n",
    "Once you have all the data, unzip it into a dir named \"data\" in the same directory of this jupyter notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets import some stuff\n",
    "import tensorflow as tf\n",
    "from skimage import io\n",
    "from IPython.display import Image\n",
    "#from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os import mkdir\n",
    "#from skimage.io import imsave\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "#import os\n",
    "import cv2\n",
    "\n",
    "#print(\"Installed version of tensorflow is \", tf.__version__)\n",
    "print(\"Important! tensorflow MUST be 1.2 or higher for this to work fine...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset\n",
    "\n",
    "The definition of kaggle dataset is \n",
    "\n",
    "\"The training archive contains 25,000 images of dogs and cats. Train your algorithm on these files and predict the labels for test1.zip (1 = dog, 0 = cat).\"\n",
    "\n",
    "Notice! your direcory should look like this (once you have uncompressed the data)\n",
    "```\n",
    ".\n",
    "├── cats-vs-dogs.ipynb\n",
    "└── data\n",
    "    └── train [25000 entries exceeds filelimit, not opening dir]\n",
    "            ├── cat.2976.jpg\n",
    "            ├── dog.2977.jpg\n",
    "            ├── cat.2978.jpg\n",
    "            ├── dog.2979.jpg\n",
    "            ├── ...\n",
    "\n",
    "```\n",
    "Of course images named cat.xxx.jpg are cats and thesame goes for the ones started with dogs.\n",
    "\n",
    "Lets open a few images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/train/\"\n",
    "Image(filename=DATA_DIR+'cat.42.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, lets show a dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename=DATA_DIR+'dog.42.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first obvious thing is that we are dealing here with images of different sizes, we need to make them the same size. Of course bigger sizes will mean more data, which means more processing time... For our example we will modify all images to be 100x100 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_WIDTH=100\n",
    "IMAGE_HEIGHT=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets read one cat image and transform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_cat_file = DATA_DIR+'cat.42.jpg'\n",
    "original = cv2.imread(sample_cat_file)\n",
    "print(\"Original shape is\", original.shape)\n",
    "transformed = cv2.resize(original, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "print(\"Transformed shape is\", transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems fine... lets convince ourselves it is the same image by displaying it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(transformed, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image has been resized, of course this means loosing some data, well, nobody is perfect :)\n",
    "Now we will need to convert ALL our images and we will store them into a separated directory, we will use that directory for subsequent executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "CLEANED_DATA_DIR = DATA_DIR+\"cleaned/\"\n",
    "try:\n",
    "    mkdir(CLEANED_DATA_DIR)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def convert_images():\n",
    "    i = 1\n",
    "    for image_file in listdir(DATA_DIR):\n",
    "        if \".jpg\" in image_file:\n",
    "            original = cv2.imread(DATA_DIR+image_file)\n",
    "            transformed = cv2.resize(original, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "            final_file = CLEANED_DATA_DIR+image_file\n",
    "            imsave(final_file, transformed)\n",
    "            if i % 500 == 0:\n",
    "                print(\"Converted \", i, \" images so far...\")\n",
    "            i += 1\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will read the new files and load them, we will load cats and dogs separately (just for convenience), then we will split them into train, validation and test set. \n",
    "\n",
    "* Train set 80% of the images\n",
    "* Validation set 10% of the images\n",
    "* Test set 10% of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CAT_LABEL = 1\n",
    "DOG_LABEL = 0\n",
    "\n",
    "def load_data(limit=None):\n",
    "    \"\"\"\n",
    "    Gets all the data and returns\n",
    "    X_train_cats, y_train_cats\n",
    "    X_train_dogs, y_train_dogs\n",
    "    X_validation_cats, y_validation_cats\n",
    "    X_validation_dogs, y_validation_dogs\n",
    "    X_test_cats, y_test_cats\n",
    "    X_test_dogs, y_test_dogs\n",
    "    \"\"\"\n",
    "    dogs = []\n",
    "    cats = []\n",
    "    i = 1\n",
    "    if limit == None:\n",
    "        limit = 50000\n",
    "    for image_file in listdir(CLEANED_DATA_DIR):        \n",
    "        if \".jpg\" in image_file:\n",
    "            if \"dog\" in image_file and len(dogs) < limit:\n",
    "                raw_image = io.imread(CLEANED_DATA_DIR+image_file)\n",
    "                dogs.append(raw_image)\n",
    "            elif \"cat\" in image_file and len(cats) < limit:\n",
    "                raw_image = io.imread(CLEANED_DATA_DIR+image_file)\n",
    "                cats.append(raw_image)\n",
    "        if i % 500 == 0:\n",
    "            print(\"Loaded \", i, \" images so far...\")\n",
    "        i+=1    \n",
    "            \n",
    "    return cats, dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cats, dogs = load_data(7500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to split our data into train, validation and test sets. As there are as many cats as there are dogs, we want to keep our sets balanced, with approximately 50% of cats and 50% of dogs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_limit = int(len(cats) * 0.8)\n",
    "validation_limit = train_limit + int(len(cats) * 0.1)\n",
    "\n",
    "X_train_cats = np.array(cats[:train_limit])\n",
    "X_train_dogs = np.array(dogs[:train_limit])\n",
    "X_validation_cats = np.array(cats[train_limit:validation_limit])\n",
    "X_validation_dogs = np.array(dogs[train_limit:validation_limit])\n",
    "X_test_cats = np.array(cats[validation_limit:])\n",
    "X_test_dogs = np.array(dogs[validation_limit:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to create our labels, remember that we have dined our labels as variables \n",
    "```\n",
    "CAT_LABEL = 1\n",
    "DOG_LABEL = 0\n",
    "```\n",
    "\n",
    "Meaning that cats are labelled as 1 and dogs as 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_cats = np.ones(len(X_train_cats), dtype=np.int8)\n",
    "y_train_dogs = np.zeros(len(X_train_dogs), dtype=np.int8)\n",
    "y_validation_cats = np.ones(len(X_validation_cats), dtype=np.int8)\n",
    "y_validation_dogs = np.zeros(len(X_validation_dogs), dtype=np.int8)\n",
    "y_test_cats = np.ones(len(X_test_cats), dtype=np.int8)\n",
    "y_test_dogs = np.zeros(len(X_test_dogs), dtype=np.int8)\n",
    "\n",
    "y_train_cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.append(X_train_cats, X_train_dogs, axis=0)\n",
    "y_train = np.append(y_train_cats, y_train_dogs)\n",
    "X_validation = np.append(X_validation_cats, X_validation_dogs, axis=0)\n",
    "y_validation = np.append(y_validation_cats, y_validation_dogs)\n",
    "X_test = np.append(X_test_cats, X_test_dogs, axis=0)\n",
    "y_test = np.append(y_test_cats, y_test_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets convert labels into one-hot-encoded values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_one_hot = np.eye(2)[y_train.reshape(-1)]\n",
    "y_validation_one_hot = np.eye(2)[y_validation.reshape(-1)]\n",
    "y_test_one_hot = np.eye(2)[y_test.reshape(-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, lets make sure the arrays are in the correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"X_train \", X_train.shape, \" y_train_one_hot \", y_train_one_hot.shape)\n",
    "print(\"X_validation \", X_validation.shape, \" y_validation_one_hot \", y_validation_one_hot.shape)\n",
    "print(\"X_test \", X_test.shape, \" y_test_one_hot \", y_test_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally shuffle the arrays so that our batches are not all dogs or all cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train_one_hot, y_train = shuffle(X_train, y_train_one_hot, y_train, \n",
    "                                            random_state=0)\n",
    "X_validation, y_validation_one_hot, y_validation = shuffle(X_validation, y_validation_one_hot, y_validation, \n",
    "                                                           random_state=0)\n",
    "X_test, y_test_one_hot, y_test = shuffle(X_test, y_test_one_hot, y_test, \n",
    "                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(y_train[0:10])\n",
    "print(y_train_one_hot[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, lets veryfy that we have shuffled correctly, we will display the first ten images of the train set and check that against the first 10 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    label = y_train_one_hot[i]\n",
    "    print(label)\n",
    "    if label[DOG_LABEL] == 1:\n",
    "        print(\"This should be a dog...\")\n",
    "        \n",
    "    elif label[CAT_LABEL] == 1:\n",
    "        print(\"This should be a cat...\")\n",
    "    \n",
    "    plt.imshow(X_train[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We still need to normalize our data, remember that normalization is done with the following formula\n",
    "![title](normalization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(raw_data):\n",
    "    min_value = np.min(raw_data)\n",
    "    max_value = np.max(raw_data)\n",
    "    result = (raw_data - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "\n",
    "X_train_normalized = normalize(X_train)\n",
    "X_validation_normalized = normalize(X_validation)\n",
    "X_test_normalized = normalize(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And remove some data we have in memory but we do not need anymore\n",
    "del cats\n",
    "del dogs\n",
    "del X_train\n",
    "del X_test\n",
    "del X_validation\n",
    "del X_train_cats \n",
    "del X_train_dogs\n",
    "del X_validation_cats\n",
    "del X_validation_dogs\n",
    "del X_test_cats\n",
    "del X_test_dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_normalized[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning! (at last!)\n",
    "\n",
    "Things to do are\n",
    "\n",
    "* Define hyperparameters\n",
    "* Build the network itself\n",
    "  * Placeholder definitions\n",
    "  * Code perse\n",
    "* Write the training code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FULLY_CONNECTED_LAYER_1 = 1024\n",
    "FULLY_CONNECTED_LAYER_2 = 1024\n",
    "CONVOLUTION_1_OUTPUT = 16\n",
    "CONVOLUTION_2_OUTPUT = 32\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "TOTAL_BATCHES = X_train_normalized.shape[0] // BATCH_SIZE\n",
    "LABELS = 2 # Either cats or dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the network itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, IMAGE_WIDTH, IMAGE_HEIGHT, 3), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, LABELS), name=\"y\")\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First convolution, turn the image into 100x100xCONVOLUTION_1_OUTPUT\n",
    "\n",
    "convolution_1 = tf.layers.conv2d(X,    \n",
    "                                 CONVOLUTION_1_OUTPUT, # Output size\n",
    "                                 (3,3), # Kernel/patch size\n",
    "                                 strides=(1,1), \n",
    "                                 padding=\"SAME\",\n",
    "                                 activation=tf.nn.relu)\n",
    "\n",
    "#Max pool to reduce image from 100x100xCONVOLUTION_1_OUTPUT to 33x33xCONVOLUTION_1_OUTPUT \n",
    "\n",
    "convolution_1 = tf.layers.max_pooling2d(convolution_1, \n",
    "                                        3,  # Kernel/patch size \n",
    "                                        3,  # Strides, this will effectively shrink the output dimension, making \n",
    "                                            # it 100 / 3 = 33.3 ~ 34\n",
    "                                        padding=\"SAME\")\n",
    "\n",
    "# Second convolution, turn the image into 34x34xCONVOLUTION_2_OUTPUT\n",
    "convolution_2 = tf.layers.conv2d(convolution_1,    \n",
    "                                 CONVOLUTION_2_OUTPUT, # Output size\n",
    "                                 (3,3), # Kernel/patch size\n",
    "                                 strides=(1,1), \n",
    "                                 padding=\"SAME\",\n",
    "                                 activation=tf.nn.relu)\n",
    "\n",
    "#Max pool to reduce image from 34x34xCONVOLUTION_2_OUTPUT to 11x11xCONVOLUTION_2_OUTPUT\n",
    "convolution_2 = tf.layers.max_pooling2d(convolution_2, \n",
    "                                        3,  # Kernel/patch size \n",
    "                                        3,  # Strides, this will effectively shrink the output dimension, making \n",
    "                                            # it 34 / 2 = 12\n",
    "                                        padding=\"SAME\")\n",
    "\n",
    "\n",
    "# So the output of the convolution is 12x12x32 = 4608, lets use that for a \"normal\" neural network\n",
    "\n",
    "fully_connected_1 = tf.layers.dense(tf.reshape(convolution_2, (-1, 12*12*32)),\n",
    "                                FULLY_CONNECTED_LAYER_1, \n",
    "                                activation=tf.nn.relu)\n",
    "\n",
    "fully_connected_1 = tf.nn.dropout(fully_connected_1, keep_prob)\n",
    "\n",
    "fully_connected_2 = tf.layers.dense(fully_connected_1,\n",
    "                                FULLY_CONNECTED_LAYER_2, \n",
    "                                activation=tf.nn.relu)\n",
    "\n",
    "fully_connected_2 = tf.nn.dropout(fully_connected_2, keep_prob)\n",
    "\n",
    "predictions = tf.layers.dense(fully_connected_2, \n",
    "                              LABELS)  \n",
    "\n",
    "softmax_calc = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predictions)\n",
    "cost = tf.reduce_mean(softmax_calc)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer().minimize(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "last_accuracy = 0\n",
    "start_at = 0\n",
    "dropout_training = 0.5\n",
    "dropout_predicting = 1.0\n",
    "debug = True\n",
    "for epoch_no in range(EPOCHS):\n",
    "    for batch_no in range(TOTAL_BATCHES):\n",
    "        start_at = batch_no\n",
    "        slice_start = start_at*BATCH_SIZE\n",
    "        slice_end = slice_start + BATCH_SIZE\n",
    "        if slice_start > len(X_train_normalized) or slice_end > len(X_train_normalized):\n",
    "            start_at = 0\n",
    "            slice_start = start_at*BATCH_SIZE\n",
    "            slice_end = slice_start + BATCH_SIZE\n",
    "            print(\"From \", slice_start, \"--\", slice_end)\n",
    "    \n",
    "    \n",
    "        my_X = X_train_normalized[slice_start:slice_end]\n",
    "        my_y = y_train_one_hot[slice_start:slice_end]\n",
    "\n",
    "        if debug:\n",
    "            print(\"Convolution_1\", convolution_1.eval(feed_dict={X: my_X, \n",
    "                                                                 y: my_y}).shape)\n",
    "            print(\"Convolution_2\", convolution_2.eval(feed_dict={X: my_X, \n",
    "                                                                 y: my_y}).shape)\n",
    "            print(\"Fully connected_1\", fully_connected_1.eval(feed_dict={X: my_X, \n",
    "                                                                 y: my_y,\n",
    "                                                                keep_prob: dropout_training}).shape)\n",
    "            print(\"Fully connected_2\", fully_connected_2.eval(feed_dict={X: my_X, \n",
    "                                                                 y: my_y,\n",
    "                                                                keep_prob: dropout_training}).shape)\n",
    "            \n",
    "            debug = False\n",
    "\n",
    "        train_step.run(feed_dict={X: my_X, \n",
    "                                  y: my_y,\n",
    "                                  keep_prob: dropout_training})\n",
    "        if batch_no % 100 == 0:\n",
    "            correct_prediction_val = tf.equal(tf.argmax(y_validation_one_hot,1), tf.argmax(predictions, 1))\n",
    "            accuracy_val = tf.reduce_mean(tf.cast(correct_prediction_val, tf.float32))\n",
    "            \n",
    "            correct_prediction_train_batch = tf.equal(tf.argmax(my_y,1), tf.argmax(predictions, 1))\n",
    "            accuracy_train_batch = tf.reduce_mean(tf.cast(correct_prediction_train_batch, tf.float32))\n",
    "            \n",
    "            print(\"Epoch \", epoch_no, \n",
    "                  \" batch number \", batch_no, \n",
    "                  \" cost \", cost.eval(feed_dict={X: my_X, \n",
    "                                                 y: my_y,\n",
    "                                                 keep_prob: dropout_training}),\n",
    "                  \" \\tval accuracy \", accuracy_val.eval(feed_dict={X: X_validation_normalized, \n",
    "                                                                   y: y_validation_one_hot,\n",
    "                                                                   keep_prob: dropout_predicting}),\n",
    "                  \"\\ttrain accuracy\", accuracy_train_batch.eval(feed_dict={X: my_X, \n",
    "                                                                           y: my_y,\n",
    "                                                                           keep_prob: dropout_training}))\n",
    "\n",
    "print(\"DONE!!\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
