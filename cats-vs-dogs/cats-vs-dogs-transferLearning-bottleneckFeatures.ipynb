{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cats vs dogs kaggle competition\n",
    "### Using VGG16 and VGG19 with bottleneck features\n",
    "#### This is largely based in this article https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html and http://www.codesofinterest.com/2017/08/bottleneck-features-multi-class-classification-keras.html\n",
    "\n",
    "First, note that we will expect datato be in this format \n",
    "\n",
    "```\n",
    "├── real_test\n",
    "│   ├── 10111.jpg\n",
    "│   ├── 8111.jpg\n",
    "│   └── 9111.jpg\n",
    "├── test\n",
    "│   ├── cat\n",
    "│   │   └── cat.4111.jpg\n",
    "│   └── dog\n",
    "│       ├── dog.1111.jpg\n",
    "│       └── dog.2111.jpg\n",
    "├── train\n",
    "│   ├── cat\n",
    "│   │   ├── cat.10111.jpg\n",
    "│   │   ├── cat.11111.jpg\n",
    "│   │   ├── cat.1111.jpg\n",
    "│   └── dog\n",
    "│       ├── dog.10111.jpg\n",
    "│       ├── dog.111.jpg\n",
    "└── validation\n",
    "    ├── cat\n",
    "    │   ├── cat.5111.jpg\n",
    "    │   └── cat.7111.jpg\n",
    "    └── dog\n",
    "        ├── dog.11111.jpg\n",
    "        └── dog.12111.jpg\n",
    "        ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import some stuff\n",
    "import tensorflow as tf\n",
    "from skimage import io\n",
    "from IPython.display import Image\n",
    "from PIL import Image as PILImage\n",
    "import pandas as pd\n",
    "\n",
    "#from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os import mkdir\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "#import os\n",
    "import cv2\n",
    "import os.path\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.applications import vgg16, vgg19\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
    "from keras.models import Model\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical  \n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "\n",
    "FLOYDHUB=False\n",
    "if FLOYDHUB:\n",
    "    OUTPUT_DIR = \"/output/\"\n",
    "    DATA_DIR = \"/input/\"\n",
    "    CLEANED_DATA_DIR = \"/input/\"\n",
    "    CATS_DIR = \"/input/train/cat/\"\n",
    "else:\n",
    "    CATS_DIR = \"data/train/\"\n",
    "    OUTPUT_DIR = \"/tmp/\"\n",
    "    DATA_DIR = \"data/\"\n",
    "    CLEANED_DATA_DIR = \"data/cleaned/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19999 images belonging to 2 classes.\n",
      "Found 2500 images belonging to 2 classes.\n",
      "Found 2501 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for validation:\n",
    "# only rescaling\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# and the same for the test set\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        CLEANED_DATA_DIR + '/train',  # this is the target directory\n",
    "        target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None)  # this means our generator will only yield batches of data, no labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        CLEANED_DATA_DIR + '/validation',\n",
    "        target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None)\n",
    "\n",
    "# And the generator for test data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        CLEANED_DATA_DIR + '/test',\n",
    "        target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_samples = 20000 # Remember, we use 80% of 25k images, so 25000 * 0.8 = 20000\n",
    "validation_samples = 2500 # Remember we use 10% of 25k images so 25000 * 0.1 = 2500\n",
    "\n",
    "test_samples = 2500\n",
    "total_steps = test_samples // batch_size\n",
    "\n",
    "training_samples = batch_size*25 # Remember, we use 80% of 25k images, so 25000 * 0.8 = 20000\n",
    "validation_samples = batch_size*10 # Remember we use 10% of 25k images so 25000 * 0.1 = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need a couple of callbacks for our models, one to perform early stopping, the other for tensorboard info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopping=EarlyStopping(monitor='val_loss', \n",
    "                             min_delta=0.001, \n",
    "                             patience=3, \n",
    "                             verbose=1, \n",
    "                             mode='auto')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./tensorboard-logs', \n",
    "                          histogram_freq=0, \n",
    "                          write_graph=True, \n",
    "                          write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_vgg16 = vgg16.VGG16(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 352s 14s/step\n"
     ]
    }
   ],
   "source": [
    "bottleneck_features_train = model_vgg16.predict_generator(train_generator, \n",
    "                                                          training_samples // batch_size, \n",
    "                                                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(OUTPUT_DIR+'bottleneck_features_train.npy', bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 7, 7, 512)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck_features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 186s 19s/step\n"
     ]
    }
   ],
   "source": [
    "bottleneck_features_validation = model_vgg16.predict_generator(validation_generator, \n",
    "                                                               validation_samples // batch_size,\n",
    "                                                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(OUTPUT_DIR+'bottleneck_features_validation.npy', bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 7, 7, 512)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck_features_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we load the bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 7, 7, 512)\n",
      "(160, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "train_data_bottleneck = np.load(OUTPUT_DIR+'bottleneck_features_train.npy')\n",
    "validation_data_bottleneck = np.load(OUTPUT_DIR+'bottleneck_features_validation.npy')\n",
    "\n",
    "print(train_data_bottleneck.shape)\n",
    "print(validation_data_bottleneck.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also need to get the labels for the features we have loaded... and we need to make sure we get them in the same order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19999 images belonging to 2 classes.\n",
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# This is the same as the train generator... but it will have the classes too.\n",
    "train_generator_classes = train_datagen.flow_from_directory(\n",
    "                            CLEANED_DATA_DIR + '/train',  # this is the target directory\n",
    "                            target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # all images will be resized to 150x150\n",
    "                            batch_size=batch_size,\n",
    "                            class_mode='categorical')  # since we use categorical_crossentropy loss, \n",
    "                                                       # we will need one-hot-encoded...\n",
    "    \n",
    "# This is the same as the validation generator... but it will have the classes too\n",
    "validation_generator_classes = validation_datagen.flow_from_directory(\n",
    "                            CLEANED_DATA_DIR + '/validation',\n",
    "                            target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "                            batch_size=batch_size,\n",
    "                            class_mode='categorical')  # since we use categorical_crossentropy loss, \n",
    "                                                       # we will need one-hot-encoded...    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we used variables\n",
    "* training_samples\n",
    "* validation_samples\n",
    "\n",
    "before to determine how many images will be for training/validation, lets use the same ones to restore the labels\n",
    "\n",
    "This is a bit non-direct, but here's how it works. Essentially we will iterate over the train_generator_classes variable a total of\n",
    "\n",
    "```python\n",
    "training_samples // batch_size\n",
    "```\n",
    "\n",
    "And the same will be done with the validation_generator_classes, that will yield us X and y elements, we can then use the y element in conjunction with ```classes``` and ```class_indices``` attributes to build a numpy array with the correct labels\n",
    "\n",
    "Also, regarding how the array is constructed, you REALLY want to check this http://akuederle.com/create-numpy-array-with-for-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_bottleneck shape is  (400, 7, 7, 512)\n",
      "train_labels shape is (400, 2)\n",
      "validation_data_bottleneck shape is  (160, 7, 7, 512)\n",
      "validation_labels shape is (160, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data_bottleneck shape is \", train_data_bottleneck.shape)\n",
    "train_labels = np.empty((0, num_classes))\n",
    "total_iterations = training_samples // batch_size\n",
    "for x, y in train_generator_classes:\n",
    "    train_labels = np.append(train_labels, y, axis=0)\n",
    "    total_iterations-=1\n",
    "    if total_iterations == 0:\n",
    "        break\n",
    "        \n",
    "print(\"train_labels shape is\", train_labels.shape)\n",
    "\n",
    "\n",
    "print(\"validation_data_bottleneck shape is \", validation_data_bottleneck.shape)\n",
    "validation_labels = np.empty((0, num_classes))\n",
    "total_iterations = validation_samples // batch_size\n",
    "for x, y in validation_generator_classes:\n",
    "    validation_labels = np.append(validation_labels, y, axis=0)\n",
    "    total_iterations-=1\n",
    "    if total_iterations == 0:\n",
    "        break\n",
    "        \n",
    "print(\"validation_labels shape is\", validation_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 6,423,298\n",
      "Trainable params: 6,423,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 400 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 7.7385 - acc: 0.4975 - val_loss: 8.7642 - val_acc: 0.4562\n",
      "Epoch 2/50\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 8.0188 - acc: 0.5025 - val_loss: 8.7642 - val_acc: 0.4562\n",
      "Epoch 3/50\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 8.0188 - acc: 0.5025 - val_loss: 8.7642 - val_acc: 0.4562\n",
      "Epoch 4/50\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 8.0188 - acc: 0.5025 - val_loss: 8.7642 - val_acc: 0.4562\n",
      "Epoch 5/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 8.0188 - acc: 0.5025 - val_loss: 8.7642 - val_acc: 0.4562\n",
      "Epoch 6/50\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 8.0188 - acc: 0.5025 - val_loss: 8.7642 - val_acc: 0.4562\n",
      "Epoch 7/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 8.0188 - acc: 0.5025 - val_loss: 8.7642 - val_acc: 0.4562\n",
      "Epoch 8/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 7.5260 - acc: 0.5150 - val_loss: 1.5800 - val_acc: 0.5250\n",
      "Epoch 9/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 6.8158 - acc: 0.4975 - val_loss: 6.9269 - val_acc: 0.4562\n",
      "Epoch 10/50\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 6.1940 - acc: 0.5175 - val_loss: 7.9198 - val_acc: 0.4562\n",
      "Epoch 11/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 6.2458 - acc: 0.5075 - val_loss: 7.3539 - val_acc: 0.5437\n",
      "Epoch 12/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 5.5492 - acc: 0.5500 - val_loss: 6.9387 - val_acc: 0.5437\n",
      "Epoch 13/50\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 4.6569 - acc: 0.5675 - val_loss: 3.8263 - val_acc: 0.5312\n",
      "Epoch 14/50\n",
      "400/400 [==============================] - 3s 8ms/step - loss: 3.8790 - acc: 0.5825 - val_loss: 2.6400 - val_acc: 0.5125\n",
      "Epoch 15/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 2.1681 - acc: 0.6400 - val_loss: 1.3002 - val_acc: 0.4750\n",
      "Epoch 16/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 0.9586 - acc: 0.6925 - val_loss: 1.3544 - val_acc: 0.3937\n",
      "Epoch 17/50\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.7431 - acc: 0.7325 - val_loss: 1.1152 - val_acc: 0.5250\n",
      "Epoch 18/50\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.6199 - acc: 0.7525 - val_loss: 2.2050 - val_acc: 0.5437\n",
      "Epoch 19/50\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.7618 - acc: 0.7025 - val_loss: 1.3265 - val_acc: 0.5312\n",
      "Epoch 20/50\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.5297 - acc: 0.8150 - val_loss: 2.3299 - val_acc: 0.4437\n",
      "Epoch 21/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 0.5936 - acc: 0.8100 - val_loss: 1.0536 - val_acc: 0.5563\n",
      "Epoch 22/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 0.4230 - acc: 0.8425 - val_loss: 3.1047 - val_acc: 0.5375\n",
      "Epoch 23/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 0.5149 - acc: 0.8375 - val_loss: 1.5316 - val_acc: 0.4250\n",
      "Epoch 24/50\n",
      "400/400 [==============================] - 3s 8ms/step - loss: 0.2950 - acc: 0.8925 - val_loss: 1.3016 - val_acc: 0.5188\n",
      "Epoch 25/50\n",
      "400/400 [==============================] - 3s 8ms/step - loss: 0.4045 - acc: 0.8675 - val_loss: 2.1575 - val_acc: 0.5250\n",
      "Epoch 26/50\n",
      "400/400 [==============================] - 3s 8ms/step - loss: 0.3288 - acc: 0.8825 - val_loss: 3.2254 - val_acc: 0.5188\n",
      "Epoch 27/50\n",
      "400/400 [==============================] - 3s 8ms/step - loss: 0.3997 - acc: 0.8750 - val_loss: 1.5801 - val_acc: 0.5000\n",
      "Epoch 28/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 0.1610 - acc: 0.9400 - val_loss: 1.6828 - val_acc: 0.5312\n",
      "Epoch 29/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 0.2343 - acc: 0.8975 - val_loss: 2.0421 - val_acc: 0.4375\n",
      "Epoch 30/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 0.2444 - acc: 0.9250 - val_loss: 1.7800 - val_acc: 0.4750\n",
      "Epoch 31/50\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.2490 - acc: 0.9125 - val_loss: 2.2100 - val_acc: 0.4500\n",
      "Epoch 32/50\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 0.3196 - acc: 0.9425 - val_loss: 2.0047 - val_acc: 0.5188\n",
      "Epoch 33/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 0.1245 - acc: 0.9525 - val_loss: 1.9383 - val_acc: 0.4938\n",
      "Epoch 34/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 0.3814 - acc: 0.9125 - val_loss: 2.4592 - val_acc: 0.4313\n",
      "Epoch 35/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 0.1003 - acc: 0.9600 - val_loss: 2.2415 - val_acc: 0.5250\n",
      "Epoch 36/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 0.2433 - acc: 0.9275 - val_loss: 2.1153 - val_acc: 0.4437\n",
      "Epoch 37/50\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 0.2204 - acc: 0.9425 - val_loss: 3.7862 - val_acc: 0.5437\n",
      "Epoch 38/50\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 0.1666 - acc: 0.9425 - val_loss: 2.1336 - val_acc: 0.5188\n",
      "Epoch 39/50\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0646 - acc: 0.9775 - val_loss: 2.9113 - val_acc: 0.5125\n",
      "Epoch 40/50\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 0.1483 - acc: 0.9600 - val_loss: 6.0372 - val_acc: 0.5437\n",
      "Epoch 41/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 0.2829 - acc: 0.9275 - val_loss: 2.5366 - val_acc: 0.4750\n",
      "Epoch 42/50\n",
      "400/400 [==============================] - 3s 8ms/step - loss: 0.0541 - acc: 0.9850 - val_loss: 4.0931 - val_acc: 0.5312\n",
      "Epoch 43/50\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 0.0721 - acc: 0.9775 - val_loss: 3.7423 - val_acc: 0.5062\n",
      "Epoch 44/50\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 0.3431 - acc: 0.9050 - val_loss: 2.6618 - val_acc: 0.5125\n",
      "Epoch 45/50\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 0.0729 - acc: 0.9750 - val_loss: 2.8807 - val_acc: 0.4938\n",
      "Epoch 46/50\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1327 - acc: 0.9575 - val_loss: 3.0278 - val_acc: 0.4688\n",
      "Epoch 47/50\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.1115 - acc: 0.9800 - val_loss: 3.2462 - val_acc: 0.5062\n",
      "Epoch 48/50\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.1031 - acc: 0.9725 - val_loss: 3.3354 - val_acc: 0.5312\n",
      "Epoch 49/50\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.1239 - acc: 0.9575 - val_loss: 3.4055 - val_acc: 0.4313\n",
      "Epoch 50/50\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.2001 - acc: 0.9400 - val_loss: 3.8768 - val_acc: 0.5188\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_data_bottleneck.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(train_data_bottleneck, train_labels,\n",
    "          epochs=50,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(validation_data_bottleneck, validation_labels))\n",
    "model.save_weights('bottleneck_fc_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
